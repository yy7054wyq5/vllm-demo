import argparse
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI(title="vLLM AI API Server", version="1.0")
llm = None  # å…¨å±€LLMå¼•æ“å®ä¾‹
sampling_params = None  # å…¨å±€é‡‡æ ·å‚æ•°

# å®šä¹‰è¯·æ±‚ä½“æ ¼å¼ï¼ˆå…¼å®¹OpenAIæ ¼å¼ï¼‰
class ChatRequest(BaseModel):
    messages: List[Dict[str, str]]
    temperature: float = 0.7
    max_tokens: int = 512
    stream: bool = False

# å¥åº·æ£€æŸ¥æ¥å£
@app.get("/health")
async def health_check():
    if llm is None:
        raise HTTPException(status_code=500, detail="Model not loaded")
    return {
        "status": "running",
        "model": llm.model_config.model,
        "gpu_memory_utilization": llm.engine_args.gpu_memory_utilization
    }

# æ ¸å¿ƒèŠå¤©æ¥å£ï¼ˆéæµå¼ï¼‰
@app.post("/v1/chat/completions")
async def chat_completion(req: ChatRequest):
    try:
        # æå–ç”¨æˆ·æœ€åä¸€æ¡æ¶ˆæ¯ï¼ˆå…¼å®¹å¤šè½®å¯¹è¯ï¼‰
        user_prompt = req.messages[-1]["content"]
        # è®¾ç½®é‡‡æ ·å‚æ•°
        params = SamplingParams(
            temperature=req.temperature,
            max_tokens=req.max_tokens
        )
        # æ¨ç†ç”Ÿæˆ
        outputs = llm.generate([user_prompt], params)
        response_text = outputs[0].outputs[0].text
        
        # å…¼å®¹OpenAIå“åº”æ ¼å¼
        return {
            "id": f"cmpl-{outputs[0].request_id}",
            "object": "chat.completion",
            "created": int(outputs[0].created_time.timestamp()),
            "model": llm.model_config.model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": len(outputs[0].prompt_token_ids),
                "completion_tokens": len(outputs[0].outputs[0].token_ids),
                "total_tokens": len(outputs[0].prompt_token_ids) + len(outputs[0].outputs[0].token_ids)
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")

def main():
    parser = argparse.ArgumentParser(description="vLLM FastAPI Server (All Versions)")
    parser.add_argument("--model", type=str, default="Qwen/Qwen3-0.6B", help="Model name/path")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Bind address")
    parser.add_argument("--port", type=int, default=8000, help="Port")
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.8, help="GPU memory utilization")
    args = parser.parse_args()

    # å…¨å±€åˆå§‹åŒ–LLMå¼•æ“ï¼ˆæ‰€æœ‰vLLMç‰ˆæœ¬é€šç”¨ï¼‰
    global llm
    llm = LLM(
        model=args.model,
        gpu_memory_utilization=args.gpu_memory_utilization,
        tensor_parallel_size=1,
        trust_remote_code=True
    )

    # å¯åŠ¨FastAPIæœåŠ¡
    print(f"ğŸš€ vLLM FastAPI Server started: http://{args.host}:{args.port}")
    print(f"ğŸ“Œ Model: {args.model}")
    uvicorn.run(app, host=args.host, port=args.port)

if __name__ == "__main__":
    main()