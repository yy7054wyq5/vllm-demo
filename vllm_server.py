import argparse
from vllm.entrypoints.openai import api_server
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

def start_vllm_server(
    model_name: str = "Qwen/Qwen-1.8B-Chat",  # è½»é‡ä¸­æ–‡æ¨¡å‹ï¼Œé€‚åˆPOC
    host: str = "0.0.0.0",
    port: int = 8000,
    gpu_memory_utilization: float = 0.8,
    max_num_batched_tokens: int = 1024,
    enable_streaming: bool = True
):
    """å¯åŠ¨vLLM OpenAIå…¼å®¹APIæœåŠ¡"""
    # 1. é…ç½®å¼•æ“å‚æ•°
    engine_args = AsyncEngineArgs(
        model=model_name,
        host=host,
        port=port,
        gpu_memory_utilization=gpu_memory_utilization,  # æ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆCPUå¯å¿½ç•¥ï¼‰
        max_num_batched_tokens=max_num_batched_tokens,  # æ‰¹å¤„ç†æœ€å¤§tokenæ•°
        tensor_parallel_size=1,  # å•GPUï¼ˆå¤šGPUå¯è°ƒæ•´ï¼‰
        load_in_4bit=True,  # 4bité‡åŒ–ï¼Œé™ä½æ˜¾å­˜å ç”¨ï¼ˆCPU/GPUéƒ½é€‚ç”¨ï¼‰
        trust_remote_code=True,  # åŠ è½½è‡ªå®šä¹‰æ¨¡å‹ï¼ˆå¦‚Qwen/ChatGLMï¼‰éœ€å¼€å¯
    )

    # 2. åˆå§‹åŒ–å¼‚æ­¥å¼•æ“
    engine = AsyncLLMEngine.from_engine_args(engine_args)

    # 3. å¯åŠ¨OpenAIå…¼å®¹APIæœåŠ¡
    print(f"ğŸš€ å¯åŠ¨vLLMæœåŠ¡ï¼šæ¨¡å‹={model_name}ï¼Œåœ°å€=http://{host}:{port}")
    print(f"ğŸ“Œ APIå…¼å®¹OpenAIæ ¼å¼ï¼Œæ”¯æŒ /v1/chat/completions /v1/models ç­‰æ¥å£")
    api_server.run_server(
        engine=engine,
        engine_args=engine_args,
        host=host,
        port=port,
        allow_credentials=True,
        enable_streaming=enable_streaming  # å¼€å¯æµå¼å“åº”
    )

if __name__ == "__main__":
    # å‘½ä»¤è¡Œå‚æ•°è§£æï¼ˆæ–¹ä¾¿å¿«é€Ÿåˆ‡æ¢æ¨¡å‹/ç«¯å£ï¼‰
    parser = argparse.ArgumentParser(description="vLLM OpenAI API POC Server")
    parser.add_argument("--model", type=str, default="Qwen/Qwen-1.8B-Chat", 
                        help="æ¨¡å‹åç§°ï¼ˆHFä»“åº“åï¼Œå¦‚Llama-3-8B-Instructã€ChatGLM3-6Bï¼‰")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="ç»‘å®šåœ°å€")
    parser.add_argument("--port", type=int, default=8000, help="ç«¯å£")
    args = parser.parse_args()

    # å¯åŠ¨æœåŠ¡
    start_vllm_server(
        model_name=args.model,
        host=args.host,
        port=args.port
    )